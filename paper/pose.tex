\documentclass[twoside]{article}
\usepackage{aistats2016}
%\usepackage[accepted]{aistats2016}

\usepackage{amssymb,amsmath,natbib,graphicx,amsthm,
  setspace,sectsty,anysize,times,dsfont,enumerate}

\usepackage[svgnames]{xcolor}

\usepackage{lscape,arydshln,relsize,rotating,multirow}
\usepackage{algorithm,algorithmic}


\newtheorem{prop}{\bf Proposition}[section]
\newtheorem{theorem}{\bf Theorem}[section]
\newtheorem{definition}{\bf Definition}[section]
\newtheorem{lemma}{\bf Lemma}[section]
\newtheorem{corollary}{\bf Corollary}[section]
\DeclareMathOperator*{\argmin}{argmin}

\marginsize{1.1in}{.9in}{.3in}{1.4in}

\newcommand{\nb}{\color{blue}}
\newcommand{\dbl}{\setstretch{1.5}}
\newcommand{\sgl}{\setstretch{1.2}}

\newcommand{\bs}[1]{\boldsymbol{#1}}
\newcommand{\mc}[1]{\mathcal{#1}}
\newcommand{\mr}[1]{\mathrm{#1}}
\newcommand{\bm}[1]{\mathbf{#1}}
\newcommand{\ds}[1]{\mathds{#1}}
\newcommand{\indep}{\perp\!\!\!\perp}


\begin{document}

\twocolumn[

\aistatstitle{One-step estimator paths for concave regularization}

%\aistatsauthor{ Matt Taddy}

%\aistatsaddress{The  University of Chicago Booth School of Business}
 
]

\begin{abstract}
The statistics literature of the past 15 years has established many favorable
 properties for sparse diminishing-bias regularization: techniques which can
 roughly be understood as providing estimation under penalty functions
 spanning the range of concavity between $L_0$ and $L_1$ norms.  However,
 lasso $L_1$-regularized estimation remains the standard tool for industrial
 `Big Data' applications because of its minimal computational cost and the
 presence of easy-to-apply rules for penalty selection.   In response, this
 article proposes a simple new algorithm framework that requires no more
 computation than a lasso path: the path of one-step estimators (POSE) does
 $L_1$ penalized regression estimation on a grid of decreasing penalties, but
 adapts coefficient-specific weights to decrease as a function of the
 coefficient estimated in the previous path step.  This provides sparse
 diminishing-bias regularization at no extra cost over the fastest lasso
 algorithms. Moreover, our `gamma lasso' implementation of POSE is accompanied
 by a reliable heuristic for the fit degrees of freedom, so that
 standard information criteria can be applied in penalty selection. The
 methods are illustrated in extensive simulations and in application of
 logistic regression to evaluating the performance of hockey players.
 \end{abstract}

\section{Introduction}
\label{intro}

For regression in high-dimensions, it is useful to regularize estimation
through a penalty on coefficient size.   $L_1$  regularization \citep[i.e.,
the lasso of][]{tibshirani_regression_1996} is especially popular, with costs
that are non-differentiable at their minima and can lead to  coefficient
solutions of exactly zero.  A related approach is concave penalized
regularization (e.g. SCAD from \citealt{fan_variable_2001} or MCP from
\citealt{zhang_nearly_2010}) with cost functions that are also spiked at zero
but flatten for large values (as opposed to the constant increase of an $L_1$
norm).  This yields sparse solutions where  large non-zero values are estimated
with little bias. 

The combination of  \textit{sparsity} and \textit{diminishing-bias} 
 is appealing in many settings, and a large literature on concave
penalized estimation has developed over the past 15 years.  For example, many
authors (e.g., from \citealt{fan_variable_2001} and
\citealt{fan_nonconcave_2004})  have contributed work on their \textit{oracle
properties}, a class of results showing conditions under which coefficient
estimates through concave penalization, or in related schemes, will be the
same as if you knew the sparse `truth' (either asymptotically or with high
probability).   From an information compression perspective,  the increased
sparsity encouraged by diminishing-bias penalties (since single large
coefficients are allowed to account for the signals of other correlated
covariates) leads to lower memory, storage, and communication requirements.
Such savings are very important in distributed  computing schemes
\citep[e.g.,][]{taddy_distributed_2015}.


Unfortunately,  exact solvers for concave penalized estimation  all require
significantly more compute time than a standard lasso.  This has precluded their use in settings -- e.g., text or web-data analysis
-- where both $n$ (the number of observations) and $p$ (covariate dimension)
   are very large. As we review in Section \ref{sec:weighted},  recent
   literature recommends the use of approximate solvers. These approximations
   take the form of iteratively-weighted-$L_1$ regularization, where the
   coefficient-specific weights are based upon results from previous iterations
   of the approximate solver.  Work on one-step estimation (OSE), e.g. by
   \cite{zou_one-step_2008}, shows that even a single step of such
   weighted-$L_1$ regularization is enough to get solutions that are close to
   optimal, so long as the pre-estimates are
\textit{good enough} starting points. The crux of success is  finding starts that are, indeed, good enough.




\begin{figure*}[tbh]
\vskip -.25cm
\includegraphics[width=\textwidth]{graphs/gamlr_eg}
\vskip -.25cm
\caption{\label{gamlr_eg} Gamma lasso estimation on $n=10^3$ 
 observations of $y_i = 4 + 3x_{1i} - x_{2i} + \varepsilon_i$, where
$\varepsilon_i \stackrel{ind}{\sim} \mr{N}(0,1)$ and
$\{x_{1i},x_{2i},x_{3i}\}$ are marginally standard normal with correlation of
0.9 between covariates ($x_{3i}$ is spurious). The penalty path has $T=100$
segments, $\lambda^1 = n^{-1}\left| \sum_i x_{1i}y_i\right|$, and
$\lambda^{100} = 0.01\lambda^1$. Degrees of freedom are on top and vertical
lines mark AICc and BIC selected models (see Section
\ref{sec:select}).}
\vskip -.25cm
\end{figure*}


This article provides a complete framework for sparse diminishing-bias
regularization that combines ideas from OSE with the concept of a
\textit{regularization path} -- a general technique, most famously associated
with the LARS algorithm \citep{efron_least_2004}, that estimates a sequence of
models under decreasing amounts of regularization.  So long as the estimates do not change too quickly along the path, such  algorithms can be very fast to run and are an efficient way to obtain a high-quality \textit{set} of models to choose amongst.  

A path of one-step estimators (POSE; Algorithm \ref{posealgo})
provides $L_1$ penalized regression  on a grid of decreasing
penalties, but adapts coefficient-specific weights to decrease as a function
of the coefficient estimated in the previous path step.  POSE takes advantage
of the natural match between path algorithms and one-step estimation: OSE
relies upon inputs being close to the optimal solution, which is precisely the
setting where path algorithms are most efficient.  We formalize `close' with a novel result in Theorem \ref{thm:sparseapprox} that relates weighted-$L_1$ to $L_0$ regularization.

This framework allows us to provide 
\begin{itemize}
\item a {\it path} of coefficient fits, each element of which corresponds to sparse diminishing-bias regularization estimation under a different level of penalization; where
\item obtaining the path of coefficient fits requires no more computation than  a state-of-the-art $L_1$ regularization path algorithm; and
\item there are reliable closed-form rules for selection of the optimal penalty level along this path.
\end{itemize}
The last capability here is derived from a Bayesian interpretation for our \textit{gamma lasso} implementation of POSE from which we are able to construct heuristic information criteria for penalty selection.
We view such tools as an essential ingredient for practical applicability in large-scale industrial machine learning where, e.g., cross-validation is not always viable or advisable.


The remainder of this paper is outlined as follows.  Section \ref{sec:algos}
presents the general regularized regression problem and introduces POSE, our
path of one-step estimators algorithm, and the gamma lasso (GL), our
implemented version of POSE.  Section \ref{sec:weighted} gives an overview on
the relationship between concave and weighted-$L_1$ regularization. Section \ref{sec:select} provides a Bayesian model interpretation
for the gamma lasso, and derives from this model a set of information criteria
that can be applied in penalty selection along the regularization path.
Finally, we present two empirical studies:
an extensive  simulation experiment in Section \ref{sec:sim}, and in Section \ref{sec:nhl} we investigate the data analysis question: given all goals in the past decade of NHL hockey, what can we say about individual player contributions? 


\section{Paths of one-step estimators}
\label{sec:algos}

Denote $n$ response observations as $\bm{y} = [y_1,\ldots,y_n]'$ and the associated matrix of $p$ covariates as $\bm{X} =
[\bm{x}_1 \cdots \bm{x}_n]'$, with rows $\bm{x}_i = [x_{i1},\ldots,x_{ip}]'$ and columns $\bs{x}_j = [x_{1j},\ldots,x_{nj}]'$.\footnote{Since the size of penalized $\beta_j$ depends upon the units of $x_{ij}$,  it is common to scale
the coefficient by $\mr{sd}(\bs{x}_j)$, the standard deviation of the $j^{th}$ column
of $\bm{X}$; this is achieved if $x_{ij}$ is replaced by $x_{ij}/\mr{sd}(\bs{x}_j)$
throughout.} Write $\eta_{i} =
\alpha+\bm{x}_i'\bs{\beta}$ as the linear equation for observation $i$, and
denote with $l(\alpha, \bs{\beta}) = l(\bs{\eta})$  the negative log likelihood.  For example, in Gaussian (linear)
regression, $l(\bs{\eta})$ is the sum-of-squares $0.5\sum_i \left(y_i - \eta_i\right)^2$ and in binomial (logistic)
regression,  $l(\bs{\eta}) = -\sum_i \left[\eta_iy_i -
\log(1+e^{\eta_i})\right]$ for $y_i \in [0,1]$. A penalized estimator is  the
solution to
\begin{equation} \label{pendev}
\argmin_{\alpha,\beta_j\in\ds{R}}~~\left\{~l(\alpha,{\bs{\beta}}) + n\lambda \sum_{j=1}^p c(|\beta_j|)~\right\},
\end{equation}
where $\lambda>0$ controls overall penalty magnitude and  $c()$ is the  coefficient cost function.
 
A few common cost functions are: 
$L_2$ $c(|\beta|) \propto \beta^2$ \citep[ridge,][]{hoerl_ridge_1970}, $L_1$ $c(|\beta|) \propto |\beta|$ \citep[lasso,][]{tibshirani_regression_1996}, the `elastic net' mixture of $L_1$ and $L_2$ \citep{zou_regularization_2005}, and the log penalty $c(|\beta|) \propto \log(1+\gamma|\beta|)$ \citep{candes_enhancing_2008}.  Those that have
a non-differentiable spike at zero (all but ridge) lead to sparse estimators,
with some coefficients set to exactly zero.   The curvature of the penalty
away from zero dictates then the weight of shrinkage imposed on the nonzero
coefficients:  $L_2$ costs increase with coefficient size,  lasso's $L_1$
penalty has zero curvature and imposes constant shrinkage, and as curvature
goes towards $-\infty$ one approaches the $L_0$ penalty of subset selection.
In this article we are primarily interested in {\it concave cost functions},
like the log penalty, which span the range between $L_1$ and $L_0$ penalties.

Penalty size, $\lambda$, acts as a {\it squelch}: it suppresses noise to
focus on the true input signal. Large $\lambda$ lead to very simple 
model estimates, while as $\lambda \rightarrow 0$ we approach maximum
likelihood estimation (MLE). Since you don't know optimal $\lambda$,
practical application of penalized estimation requires a {\it regularization
path}: a $p \times T$ field of $\bs{\hat\beta}$ estimates, say $\bs{\hat\beta}\vert_{\lambda}$, obtained while
moving from high to low penalization along $\lambda^1 > \lambda^2 \ldots >
\lambda^T$.  These paths begin at $\lambda^1$ set to infimum $\lambda$ such that
(\ref{pendev}) is minimized at $\bs{\hat\beta}\vert_{\lambda} = \bm{0}$, and proceed down to some pre-specified $\lambda^T$ (e.g., $\lambda^T=
0.01\lambda^1$).

Our path of one-step estimators (POSE) framework is in  Algorithm \ref{posealgo}.   In this, we are assuming a penalty specification such that $\lim_{b\to 0} c'(|b|) = 1$ and that the cost function is differentiable for $b\neq 0$.  

{
\begin{algorithm}[hb]
\caption{\label{posealgo} POSE }
\vskip .2cm
Initialize $\lambda^1 = \mathrm{inf}\left\{\lambda:~ \bs{\hat\beta}\vert_{\lambda} = \bf{0}\right\}$, so that $\bs{\hat\beta}_1 = \bf{0}$.

\vskip .2cm
Set step size
$0 < \delta < 1$.

\vspace{-.75cm}
\begin{align}
\text{for}~t=2\ldots T :&\notag \\
\lambda^{t} &= \delta \lambda^{t-1}\notag\\
\omega^{t}_j  &=  
\left\{ 
  \begin{array}{r}
    c'(|\hat\beta^{t-1}_j|) ~\text{for}~j \in \hat S_t \\
    1  ~\text{for}~j \in \hat S_t^c  
  \end{array} 
  \right. 
  \label{wset}\\
\left[\hat\alpha,\bs{\hat\beta}\right]^t &= \argmin_{\alpha,\beta_j\in\ds{R}}~~
l(\alpha,\bs{\beta}) + n\sum_j \lambda^t\omega^t_j|\beta_j| \label{l1pen}
\end{align}
\vskip -.3cm
\end{algorithm}}

Section \ref{sec:weighted} will detail how POSE relates to concave regularization.  However, for some quick intuition, consider POSE with a concave cost function (such as the log penalty in Figure \ref{solution}).
The derivative $c'(|\hat\beta|)$ will be positive but decreasing with larger values of  $\hat\beta$, such that the \textit{weight} on the $L_1$ penalty for $\hat\beta_j^{t}$ will \textit{diminish} with the size of $|\hat\beta_j^{t}|$.  This implies that coefficient estimates later in the path will be less biased towards zero if that coefficient has a large value earlier in the path.


\subsection{The gamma lasso}
\label{sec:gamlr}

The gamma lasso (GL) specification for POSE is based upon the log penalty,
\begin{equation}\label{logpen}
c(\beta_j) =  \log(1+\gamma|\beta_j|),
\end{equation} where $\gamma > 0$.  This
penalty is concave with curvature $-1/(\gamma^{-1}+|\beta_j|)^2$ and it spans the
range from $L_0$ ($\gamma\rightarrow \infty$) to $L_1$ ($\gamma\rightarrow 0$)
costs.  It appears under a variety of parameterizations and names in the
literature; see
\citet{mazumder_sparsenet_2011} and applications in
\citet{friedman_fast_2008}, \citet{candes_enhancing_2008},
\citet{cevher_learning_2009}, \citet{taddy_multinomial_2013} and \citet{armagan_generalized_2013}. 

GL simply replaces line (\ref{wset}) in Algorithm \ref{posealgo}  with
\begin{equation}\label{gammalasso}
\omega^{t}_j  = \left(1 + \gamma |\hat\beta^{t-1}_j|\right)^{-1} ~~j=1\ldots p 
\end{equation}
Behavior of the resulting paths  is governed by $\gamma$, which we refer to as the
penalty {\it scale}.  Under $\gamma=0$, GL is just the usual lasso.
 Bias diminishes faster for larger $\gamma$ and, at the  extreme,
$\gamma=\infty$ yields a subset selection routine where a coefficient is
unpenalized in all segments after it first becomes nonzero. Figure
\ref{gamlr_eg} shows solutions in a simple problem.


Each  gamma lasso path segment is solved through coordinate descent (see supplement).  The algorithm is implemented in {\tt
c} as part of the {\tt gamlr} package for {\sf R}. 
%The software  has detailed documentation and  source code is at  {\tt github.com/mataddy/gamlr}.  
Usage of {\tt gamlr} mirrors that of its
convex penalty analogue {\tt glmnet} \citep{friedman_regularization_2010}, the
fantastic and widely used package for costs between $L_1$ and $L_2$ norms. In
the lasso case ($\gamma=0$), the two algorithms are essentially equivalent.


\section{Weighted-$L_1$ approximations to concave penalization}
\label{sec:weighted}

\begin{figure*}[t]
\vskip -.25cm
\includegraphics[width=\textwidth]{graphs/solution}
\vskip -.25cm
\caption{\label{solution} Log penalties $c(\beta) = s\log(1 + \gamma|\beta|)$ 
and penalized objectives $(\beta-B)^2 + c(\beta)$.}
\end{figure*}


Concave penalties such as the log penalty, which have a gradient that is
decreasing with absolute coefficient size,  yield the `diminishing-bias'
property discussed above. It is {\it the} reason why one would
use concave penalization instead of $L_1$ or convex alternatives.

Unfortunately, such penalties can overwhelm the convex likelihood and produce a
concave minimization objective; see Figure \ref{solution}.
This  makes computation difficult.  For example,  one run of SCAD via the
\texttt{ncvreg} R package \citep{breheny_coordinate_2011} for the simulation in
Section \ref{sec:sim} requires around 10 minutes, compared to 1-2 seconds for 
lasso (or gamma lasso).  The most efficient exact solver that we've found is the
\texttt{sparsenet} of \citet{mazumder_sparsenet_2011}, also implemented in R,
which first fits a lasso path and, for
each segment on this path, adapts coefficient estimates along a second path of
increasing penalty concavity. However, \texttt{sparsenet}  relies upon solution
over a large set of specifications\footnote{POSE shares with
\texttt{sparsenet} the idea of moving along a path of closely related
specifications, but does not require a grid in both  cost size and concavity.
Intuitively,  POSE runs  a path diagonally through this grid.}  
 and its compute cost remains much higher than for the [gamma] lasso.


Local linear approximation \cite[LLA; e.g.,][]{candes_enhancing_2008}
algorithms replace the concave cost function $c$  with its tangent at the
current estimate, $c'(\hat\beta_j)\beta_j$.  The objective is then just a
weighted $L_1$ penalized loss. An exact LLA solver iterates between updating
$c'(\hat\beta)$ and solving the implied $L_1$ penalized minimization problem.
\citet{zou_one-step_2008} present numerical and theoretical evidence that LLA
can provide near-optimal solutions even if you {\it stop it
after one iteration}. This is an example of one-step estimation (OSE), a
technique inspired by \cite{bickel_one-step_1975} that amounts to taking as
your estimator the first step of an iterative approximation to some objective.
Early-stopping can be as good  as the full solution {\it if} the
initial estimates are good enough.

OSE and similar ideas have had a resurgence in the concave penalization
literature recently, motivated by the need for faster estimation algorithms.
\cite{fan_strong_2014} consider early-stopping of LLA for folded concave
penalization  and show that, under strong sparsity assumptions about true
$\bs{\beta}$ and given appropriate initial values, OSE LLA is with high
probability an oracle estimator.   Zhang (2010,2013)
\nocite{zhang_analysis_2010,zhang_multi-stage_2013} investigates  
`convex relaxation' iterations, where estimates under convex regularization
 are the basis for weights in a subsequent penalized objective.
 \cite{wang_calibrating_2013} propose a two step algorithm that feeds lasso
 coefficients into a linear approximation to folded concave penalization.
 These OSE methods are all closely related to  the adaptive lasso
\citep[AL;][]{zou_adaptive_2006}, which does weighted-$L_1$ minimization under
weights $\omega_j = 1/|\hat\beta^0_j|$, where $\hat\beta^0_j$ is an initial
guess at the coefficient value.  The original AL paper advocates using MLE
estimates for initial values, while
\cite{huang_adaptive_2008} suggest using marginal regression coefficients
$\hat\beta^0_j = \mr{cor}(\bm{x}_j,\bm{y})$.

The main point here is that OSE LLA, or a two-step estimator starting from
$\bs{\hat\beta}=0$, or any version of the adaptive lasso, are all  interpretable as weighted-$L_1$ 
penalization with weights equal to something like $c'(\beta^0)$ for initial
coefficient guess $\beta^0$. POSE and GL take advantage of an  available
source of initial values in any path estimation algorithm -- the solved values
from the previous path iteration.  Our  simulations in Section \ref{sec:sim}
show that this efficient strategy works as well or better than  expensive
exact solvers.  In the next section, we provide some theoretical intuition on
why it works.


\subsection{Comparison between weighted-$L_1$ and $L_0$ penalized estimation}


Our oracle benchmark is estimation under $L_0$ costs, $c(\beta_j) =
\ds{1}_{\{\beta_j\neq0\}}$, for which global solution is impractical.   We are
interested in weighted-$L_1$ penalization as a way to obtain fits that are as
sparse as possible without compromising predictive ability, regardless of the
underlying data generating process (or `true' sparsity).

 For  $S \subset \{1\ldots p\}$ with cardinality $|S|=s$ and complement $S^c =
\{1\ldots p\}\setminus S$, denote vectors restricted to covariates in $S$ as
$\bm{\beta}_S = [\beta_j:j\in S]'$, matrices as $\bm{X}_S$, etc.  Use
$\bs{\beta}^S$ to denote the coefficients for ordinary least-squares (OLS)
restricted to $S$: that is, $\bs{\beta}^S_S =
(\bm{X}_S'\bm{X}_S)^{-1}\bm{X}_S'\bm{y}$ and $\beta^{S}_j = 0~\forall~j\notin
S$.  Moreover, $\bm{e}^S = \bm{y}-\bm{X}\bs{\beta}^S =
(\bm{I}-\bm{H}^S)\bm{y}$ are residuals and $\bm{H}^S =
\bm{X}_S(\bm{X}_S'\bm{X}_S)^{-1}\bm{X}_S'$ the projection matrix from OLS on $S$.  
Use $|\cdot|$ and $\|\cdot\|$ for $L_1$ and $L_2$ norms.

We use the following result for iterative \textit{stagewise} regression, with proof in the supplemental appendix. 
\begin{lemma}\label{SSElemma}
Say $\mr{MSE}_S = \|\bm{X}\bs{\beta}^S-\bm{y}\|^2/n$ and 
$\mr{cov}(\bs{x}_j,\bm{e}^S) = \bs{x}_j'(\bm{y}-\bm{X}\bs{\beta}^S)/n$ are sample variance and covariances.  Then for any $j \in 1\ldots p$, 
\[
\mr{cov}^2(\bs{x}_j,\bm{e}^S) \leq \mr{MSE}_S - \mr{MSE}_{S\cup j}
\]
\end{lemma}

In addition, we need to define {\it restricted eigenvalues} (RE) on the gram matrix $\bm{X}'\bm{X}/n$.\footnote{
  This  RE  matches the `adaptive restricted eigenvalues' of \cite{buhlmann_statistics_2011}.  
Similar quantities are common in the theory of regularized estimators; see also
\cite{raskutti_restricted_2010} and  \cite{bickel_simultaneous_2009}.}

  
\begin{definition}\label{redef}
The restricted eigenvalue is
$
\phi^2(L,S) = \min_{\{\bm{v}: \bm{v}\neq \bm{0},~|\bm{v}_{S^c}| \leq L\sqrt{s}\|\bm{v}_S\|\}}\frac{\|\bm{X}\bm{v}\|^2}{n\|\bm{v}\|^2}$.
\end{definition}

Finally,  we bound the distance between prediction rules
from $L_0$ and weighted-$L_1$ penalized estimation.  

\begin{theorem} \label{thm:sparseapprox}  Consider squared-error loss
$l(\bs{\beta}) =
\frac{1}{2}\|\bm{X}\bs{\beta}-\bm{y}\|^2$, and suppose $\bs{\beta}^{\nu}$ minimizes the $L_0$ penalized objective $l(\bs{\beta}) + n\nu\sum_{j=1}^p\ds{1}_{\{\beta_j\neq0\}}$ with $\bs{\beta}^\nu = \bs{\beta}^S$ and $|S|=s<n$.   
Write $\bs{\hat\beta}$ as solution to the weighted-$L_1$ minimization $l(\bs{\beta}) + n\lambda\sum_j\omega_j|\beta_j|$. 

Then  
$\omega^{\mr{min}}_{S^c}\lambda > \sqrt{2\nu}$ while $\phi^2(L,S) > 0$ implies
\begin{equation} \label{sparseineq}
\frac{\|\bm{X}(\bs{\hat\beta}-\bs{\beta}^\nu)\|^2}{n}\leq
\frac{4\lambda^2 \|\bs{\omega}_S\|^2}{\phi^2(L, S)}
\end{equation} 
with 
 $L = \frac{\|\bs{\omega}_S\|}{\sqrt{s}}\left(\omega^{\mr{min}}_{S^c}-\sqrt{2\nu}/\lambda\right)^{-1}$ for the RE.
\end{theorem}

\begin{proof}
From the definitions of $\bs{\hat\beta}$ and $\bs{\beta}^\nu = \bs{\beta}^S$, 
\begin{align}\label{predconv}&
\frac{1}{2}\|\bm{X}\bs{\hat \beta}-\bm{y}\|^2 +  n\lambda\sum_j\omega_j|\hat\beta_j|  \\ &= \frac{\|\bm{X}(\bs{\hat\beta}-\bs{\beta}^\nu)\|^2}{2} + \frac{\|\bm{e}^S\|^2}{2} - \bm{\hat y}'\bm{e}^S+ n\lambda\sum_j\omega_j|\hat\beta_j|\notag\\
&\hspace{3cm}\leq~~\frac{1}{2}\|\bm{e}^S\|^2 + n\lambda\sum_{j\in S}\omega_j|\beta^S_j|\notag
\end{align}
  Since $\bm{\hat y}'\bm{e}^S = \bm{\hat y}'(\bm{I}-\bm{H}^S)\bm{y} =
\bs{\hat\beta}'\bm{X}'(\bm{y}-\bm{X}\bs{\beta}^\nu) = 
\sum_{j\in S^c} \hat\beta_j\bs{x}_j'(\bm{y}-\bm{X}\bs{\beta}^\nu)
$,
 we can apply Lemma \ref{SSElemma} followed by $\bs{\beta}^\nu$ being optimal under $L_0$ penalty $\nu$ to get 
\begin{equation} \label{L0ineq}
\left(\frac{\bs{x}_j'(\bm{y}-\bm{X}\bs{\beta}^\nu)}{n}\right)^2
\leq \mr{MSE}_S - \mr{MSE}_{S\cup j} < 2\nu ~~~\forall~j
\end{equation}
so that $|\bm{\hat y}'\bm{e}^S| = |\bs{\hat\beta}_{S^c}\bm{X}_{S^c}'(\bm{y}-\bm{X}\bs{\beta}^\nu)| < n\sqrt{2\nu}|\bs{\hat\beta}_{S^c}|$.  Applying this inside (\ref{predconv}),
\begin{align}\label{predconvineq}&
\frac{1}{2}\|\bm{X}(\bs{\hat\beta}-\bs{\beta}^\nu)\|^2
  + n\left(\omega^{\mr{min}}_{S^c}\lambda-\sqrt{2\nu}\right)|\bs{\hat\beta}_{S^c}|\\
  &~\leq~ n\lambda\sum_{j\in S}\omega_j|\hat\beta_{j}-\beta^\nu_j|
  ~\leq~ n\lambda\|\bs{\omega}_S\|\|\bs{\hat\beta}_{S}-\bs{\beta}^\nu_S\|.\notag
\end{align}
Given $\omega^{\mr{min}}_{S^c}\lambda > \sqrt{2\nu}$,
difference $\bs{\hat\beta}-\bs{\beta}^\nu$ is in the RE support for 
$L=\frac{\|\bs{\omega}_S\|}{\sqrt{s}}(\omega^{\mr{min}}_{S^c}-\sqrt{2\nu}/\lambda)^{-1}$ and thus $\|\bs{\hat\beta}_{S}-\bs{\beta}^\nu_S\| \leq \|\bm{X}(\bs{\hat\beta}-\bs{\beta}^\nu)/\sqrt{n}\|/\phi(L,S)$.  Finally, applying this inside (\ref{predconvineq}) yields
\begin{align}
\frac{1}{2}\|\bm{X}(\bs{\hat\beta}-\bs{\beta}^\nu)\|^2
  &~\leq~ \frac{\sqrt{n}\lambda\|\bs{\omega}_S\|\|\bm{X}(\bs{\hat\beta}-\bs{\beta}^\nu)\|}
  {\phi(L, S)}.
\end{align}
Dividing each side by $\sqrt{n}\|\bm{X}(\bs{\hat\beta}-\bs{\beta}^\nu)\|/2$ and squaring gives the result.
\end{proof}

\noindent {\bf Remarks}


\noindent $\bullet$~~  Theorem \ref{thm:sparseapprox} is finite sample exact.  Distinguishing it from
related results in the literature, it is  also completely
\textit{non-parametric} -- it makes no reference to the true distribution of
$\bm{y}|\bm{X}$.  Indeed, if we make such assumptions, Theorem
\ref{thm:sparseapprox} provides bounds on the distance between a
weighted-lasso and optimal prediction.  The next remark is an example.

\noindent $\bullet$~~ Assume that $\bm{y} \sim
(\bs{\eta},\sigma^2\bm{I})$ --  $y_i$ independent with mean $\mu_i$ and shared
variance $\sigma^2$.  The $C_p$ formula of
\cite{mallows_comments_1973} gives 
$\mr{MSE}_S + 2s\sigma^2/n$ as an unbiased estimate of residual variance.
Following \cite{efron_estimation_2004}, this implies $\nu = \sigma^2/n$ is the
optimal $L_0$ penalty for minimizing prediction error. Theorem
(\ref{thm:sparseapprox}) applies directly,  with $L =
\frac{\|\bs{\omega}_S\|}{\sqrt{s}}\left(\omega^{\mr{min}}_{S^c}-
\sqrt{2}\sigma/(\lambda\sqrt{n})\right)^{-1}$, to give a bound on the distance
between weighted-$L_1$ estimation and $C_p$-optimal prediction. Note that,
since the condition on minimum $S^c$ weights has become
$\omega^{\mr{min}}_{S^c} > (\sigma/\lambda)\sqrt{2/n}$, comparison to $C_p$
suggests we can use larger $\gamma$  with large $n$ or small $\sigma$.


\noindent $\bullet$~~  Plausibility of the restricted eigenvalue assumption
$\phi(L,S) > 0$ depends  upon $L$.  It is less restrictive if we can reduce
$\|\bs{\omega}_S\|$ without making $\omega^{\mr{min}}_{S_c}$ small.
\textit{This is a key motivation for the POSE algorithm:} if covariates with
nonzero $\hat \beta_j$ for large $\lambda$ (i.e., early in the path) can be
assumed to live in $S$, then increasing $\gamma >0$ will improve prediction.
Of course, the moment that $\lambda$ becomes small enough that elements of
$S_c$ get nonzero $\hat\beta$, then larger $\gamma$ can lead to over-fit.  For
this reason it is essential that we have tools for choosing  optimal
$\lambda$.

\noindent $\bullet$~~ In the supplemental material,  we also adapt standard
results from \cite{wainwright_sharp_2006,wainwright_sharp_2009} to show how
reducing $\|\bs{\omega}_S\|$ without making $\omega^{\mr{min}}_{S_c}$ small
can lead to lower false discovery rates with respect to the $L_0$ oracle.
However, such results depend upon design restrictions that are less realistic
in practice.


\section{Penalty selection}
\label{sec:select}

Lasso, the gamma lasso, and related 
sparse regularization estimators do not actually {\it do} model selection; rather, they
can be used to obtain paths of estimates corresponding to different levels of
penalization.  Each penalty level corresponds to a different `model'
and we must select the optimal choice from these candidates.

$K$-fold cross-validation \cite[CV; e.g.,][]{efron_estimation_2004} 
is the most common technique for penalty selection, and it does a good job.  However, there are many scenarios where
we might want an analytic alternative to CV.  For example, if a single fit is expensive then doing it $K$
times will be impractical.  More subtly, truly Big Data are distributed: they
are too large to store on a single machine. Algorithms can be designed to work
in parallel on subsets
\citep[e.g.,][]{taddy_distributed_2015} but a bottleneck
results if you need to communicate across machines for CV experimentation.
Finally, CV can lead to over-fit for unstable algorithms whose
results change dramatically in response to data jitter; see
\cite{breiman_heuristics_1996} for a classic discussion.  Such instability
arises for large $\gamma$ combined with small $\lambda$ in our GL algorithm
and, more generally, in many concave regularized estimators; see the
supplemental material for a detailed overview.

An important feature of the standard $L_1$ lasso is that it comes with a
simple approximation for the estimation degrees-of-freedom (\textit{df}) at
any $\lambda$: the number of nonzero estimated coefficients
\citep[see][]{zou_degrees_2007}.  These \textit{df} can then combined with the fitted
deviance for penalty selection via common information criteria.

This section derives the gamma lasso as approximately maximizing the posterior
for a hierarchical Bayesian model, and uses this
interpretation to obtain a heuristic degrees-of-freedom for each estimate
along the GL path.  These GL \textit{df} can  be input to information criteria
for model selection. In particular, our extensive simulations demonstrate that the GL
\textit{df} can be used with  the corrected AICc of
\citet{hurvich_regression_1989}  to obtain out-of-sample predictive
performance that is as good or better than that from cross-validated
predictors.


\subsection{Bayesian model interpretation}

Consider a model where each $\beta_j$ is
assigned a Laplace distribution prior with scale $\tau_j>0$,
\begin{equation}\label{blasso}
\beta_j \sim \mr{La}\left(\tau_j\right) =
\frac{\tau_j}{2}\exp\left[ -\tau_j|\beta_j| ~\right].
\end{equation}
Typically, scale parameters $\tau_1 =
\ldots = \tau_p$ are set as a single value, say $n\lambda/\phi$ where
 $\phi$ is the dispersion (e.g. Gaussian variance
$\sigma^2$ or 1 for the binomial).   Posterior
maximization under the prior in (\ref{blasso}) is $L_1$ regularized estimation \citep[e.g.,][]{park_bayesian_2008}.

Instead of a single shared scale, assume an independent gamma
$\mr{Ga}(s,1/\gamma)$ hyperprior with `shape' $s$ and `scale' $\gamma$ for
each $\tau_j$, such that $\ds{E}[\tau_j] = s\gamma$ and $\mr{var}(\tau_j) =
s\gamma^2$.  The {\it joint} coefficient-scale prior is
\begin{align}\label{glprior}
\pi(\beta_j,\tau_j) &= \mr{La}\left(\beta_j ;~ \tau_j\right)
\mr{Ga}\left(\tau_j;~ s,\gamma^{-1}\right) \\&= \frac{ 1}{2\Gamma({s})} 
\left(\frac{\tau_j}{\gamma}\right)^{s}
               \exp\left[-\tau_j(\gamma^{-1}+|\beta_j|)\right].\notag
\end{align}
The gamma hyperprior is conjugate here, implying a $\mr{Ga}\left(s+1, ~1/\gamma +
|\beta_j|\right)$ posterior for $\tau_j \mid \beta_j$ with conditional
posterior mode (MAP) at $\hat\tau_j = \gamma s/(1 + \gamma |\beta_j|)$.

Consider joint MAP estimation of $[\bs{\tau},\bs{\beta}]$ under the prior in
   (\ref{glprior}), where we've suppressed $\alpha$ for simplicity. By taking
   negative logs and removing constants, this is equivalent to solving
\begin{equation}\label{gljoint}
\argmin_{\beta_j\in\ds{R},~\tau_j \in \ds{R}^{+}}\!\!
\frac{l(\bs{\beta})}{\phi} + \sum_j \left[\tau_j(\gamma^{-1}+|\beta_j|) - s\log(\tau_j)\right],
\end{equation}
and is straightforward to show (supplemental) that this is equivalent 
to the log-penalized objective 
\begin{equation}\label{logobj}
\min_{\beta_j\in\ds{R}}~~
\phi^{-1}l(\bs{\beta}) + \sum_j  s\log(1+\gamma|\beta_j|).
\end{equation}

\subsection{Degrees of freedom}


For prediction rules, say $\hat y_i$, that are suitably stable \citep[i.e.,
Lipschitz; see][]{zou_degrees_2007}, the SURE framework of
\cite{stein_estimation_1981} applies and  $df =
\ds{E}\left[\sum_i \partial \hat y_i/\partial y_i\right]$.
Consider  a single coefficient $\beta$ estimated via least-squares under $L_1$
penalty $\tau$.   Write gradient at zero $g = -\sum_i x_iy_i$ and curvature $h
= \sum_i x_i^2$ and set $\varsigma = -{\tt sign}(g)$. The prediction rule is
$\hat y = x(\varsigma/h)(|g|-\tau)_+$ with  derivative $\partial\hat y_i/\partial y = x_i^2/h \ds{1}_{[|g|<\tau]}$, so that the SURE expression
yields $df = \ds{E}\left[ \ds{1}_{[|g|<\tau]} \right]$.   This expectation is
taken with respect to the {\it unknown true} distribution over $\bm{y} |
\bm{X}$, not that estimated from the observed sample.  However, 
 one can evaluate this expression at observed
gradients as an unbiased estimator for the true \textit{df} \citep[e.g.,][]{zou_degrees_2007}.

This motivates our heuristic $df$ in weighted-$L_1$ regularization:  the {\it prior} expectation for the number  $L_1$ penalty dimensions, $\tau_j = \lambda \omega_j$, that are less than their corresponding absolute gradient dimension.  Referring to our Bayesian model above, each $\tau_j$ is $iid$ $\mr{Ga}(s,1/\gamma)$ in the prior,
leading to the GL degrees of freedom
\begin{equation}
\label{edf} df^t = \sum_j \mr{Ga}(|g_{j}|;~n\lambda^t/(\gamma\phi),
1/\gamma), \end{equation} where $\mr{Ga}(~\cdot~;~\mr{shape}, 1/\mr{scale})$
is the Gamma cumulative distribution function and $g_j$ is an estimate of  the
$j^{th}$ coefficient gradient evaluated at $\hat\beta_j=0$.\footnote{Note that the number of unpenalized variables (e.g., 1 for $\alpha$) should also be added to the total  estimation $df$. }

For  orthogonal covariates,  $g_j$ is just the marginal gradient at zero.
In the non-orthogonal case, where $g_{j} = g_j(0)$ becomes a
function of all of the elements of $\bs{\hat\beta}$, we plug in the most recent
$g_j$ at which $\hat\beta^t_j=0$:  this
requires no extra computation and has the advantage of maintaining $df =
\hat p^t$ for $\gamma = 0$.

\subsection{Selection via information criteria}

An information criterion is an attempt to approximate
 divergence between the unknown true data generating
process and our parametric approximation; see the supplement for an
overview.  These  take the form
\begin{equation}
l(\bs{\hat\beta}) + k(df)
\end{equation}
where $k$ is the cost on the degrees-of-freedom associated with
 $\bs{\hat\beta}$ and $l$ is the negative log likelihood.  The AIC of
\cite{akaike_information_1973} uses $k(df) = df$ while the BIC of \cite{schwarz_estimating_1978}
uses $k(df) = \log(n)df/2$.

As detailed in \cite{flynn_efficiency_2013}, the corrected AICc with $k(df) =
df\times n/(n-df-1)$ does a better job than the AIC or BIC in choosing the
optimal model for prediction when $df$ is large.  Alternatively, the BIC is often preferred
for accurate support recovery or avoiding false discovery; see, e.g.,
\cite{zou_degrees_2007}.


\section{Simulation experiment}
\label{sec:sim}

We consider samples of size $n=1000$ from
\begin{align}
\label{simdgp}
&y \sim \mr{N}\left(\bm{x}'\bs{\beta},\sigma^2\right)~\text{where}~\beta_j = \frac{\exp\left(-\frac{j}{\sf d}\right)}{j},~j=1\ldots p,\notag\\
&\text{and}~\bm{x} = \bm{u}*\bm{z},~~\bm{u}\sim \mr{N}\left(\bm{0},\bs{\Sigma}\right),~~z_{j} \stackrel{ind}{\sim} \mr{Bin}(0.5).
\end{align}

\vspace{-.4cm}
\noindent
Each simulation draws means $\eta_i =
\bm{x}_i'\bs{\beta}$, and two independent response samples 
$\bm{y},\bm{\tilde y} \sim \mr{N}(\bs{\eta},\sigma^2\bm{I})$. Multicollinearity is
parametrized via $\Sigma_{jk} =
\rho^{|j-k|}$ with $\rho = 0.5$.  We define $\sigma^2$ through {\it signal-to-noise} ratios
$\mr{sd}(\bs{\eta})/\sigma$ of $1/2$, $1$, and $2$; for coefficient decay rates we consider
${\sf d}$ of $10$, $50$, and $100$.\footnote{Results for many additional configurations, including different covariate sparsity and multicollinearity, are available in the supplement.}


The regression in (\ref{simdgp}) is obviously {\it dense}:  true
coefficients are all nonzero.  However, they decay in magnitude along the
index $j$ and
it will be useless to estimate many of them in a $p=n$ regression.
Our sparse oracle comparator is the $C_p$ optimal $L_0$ penalized solution

\vspace{-.3cm}
\begin{equation}\label{l0oracle}
\bs{\beta}^{\star} = \argmin_{\bs{\beta}} \left\{ \|\bm{y}-\bm{X}\bs{\beta}\|^2 + 2\sigma^2\sum_j
\ds{1}_{\{\beta_j\neq0\}}\right\},
\end{equation} which is solvable here by searching through
OLS regression on $\bm{X}_{\{1\ldots j\}}$ for $j=1\ldots p$.

\begin{table}
\footnotesize
\begin{tabular}{cc|rrrrr}
\multicolumn{2}{c}{~}&\multicolumn{4}{c}{\it \% Worse than oracle $C_p$  } &   \\
\multicolumn{2}{c}{~}&\multicolumn{3}{c}{\scriptsize\it AICc}& \multicolumn{1}{c}{\scriptsize\it CV}&\\[-1ex]
\multicolumn{2}{c}{~}&\multicolumn{3}{c}{\downbracefill}& \multicolumn{1}{c}{\it \downbracefill}&\\
$\frac{\mr{sd}(\bs{\eta})}{\sigma}$& \textsf{d} & GL0 & GL2 & GL10 & \multicolumn{1}{c|}{MCP} & $C_p$ $R^2$ 
\\[1ex]
\hline\rule{0pt}{4ex}
\!2 &  10 & 2.5 & \bf 1.3 & 3.8 & \bf 1.3 & .79\\
2 & 50 & 7.8 & \bf 3.9 & 6.5 &\bf  3.9 & .77 \\
2 & 100 & 12.0 & 8.0 & 8.0 &\bf 6.7 & .75\\
1 & 10  & 8.3 & 6.2 & 14.6 &\bf 4.2 & .48\\
1 & 50 & 27.3 &\bf 16.0 & 18.2 & 18.2 & .44\\
1 & 100 & 35.0 & 27.5 & \bf 25.0 & 27.5 & .40 \\
1/2 & 10 &33.3 &\bf  27.8 & 61.1 &\bf  27.8 & .18\\
1/2 & 50 &\bf 71.4 & 92.9 & 100.0 &\bf 71.4 & .14\\
1/2 & 100 &\bf 80.0 & 110.0 & 110.0 & 90.0 & .10
\\\hline
\end{tabular}
\end{table}



We consider {\tt gamlr} runs of GL with $\gamma$
of 0 (lasso), 2, and 10 and marginal AL, as well as
{\tt sparsenet}'s MCP penalized regression.  Covariate penalties are
standardized by $\mr{sd}(\bs{x}_j)$, and paths run through 100
$\lambda$ values down to $\lambda^{100} = 0.01\lambda^1$. On an Intel Xeon
E5-2670 core, 5-fold CV with $\mr{sd}(\bm{\eta})/\sigma=1$
and $\rho=1/2$ requires   1-2 seconds for lasso and marginal  AL, 2 and 3
seconds for GL with $\gamma=2$ and $\gamma=10$, and 15-20 seconds
for Sparsenet.
\footnote{{\tt ncvreg} SCAD required ten minutes for a single run and is thus
impractical for the applications under consideration.  But, in a small study,
CV.min selected SCAD performs quite well in prediction --  similarly to the
best CV.min methods for each data configuration -- with relatively high values
for both false discovery and sensitivity. }


Figures \ref{simpaths} and \ref{simcv} illustrate GL paths for a single
dataset, with  $\mr{sd}(\bm{\eta})/\sigma=1$ and $\rho=1/2$. In Figure
\ref{simpaths}, increasing $\gamma$ leads to `larger shouldered' paths where
estimates move quickly to MLE for the nonzero-coefficients. Degrees of
freedom, calculated as in (\ref{edf}), are along the top of each plot; equal
$\lambda$ have higher $df^t$ for higher $\gamma$ since there is less shrinkage
of $\hat\beta_j\neq0$.  Figure \ref{simcv} shows CV and AICc error estimates.
The two criteria roughly track each other, although AICc more heavily
penalizes over-fit and at $\gamma=10$ their minima do not match.  Notice that
as $\gamma$ increases, the CV error increases more quickly after it's minimum;
this indicates that the consequences of over-fit are worse under faster
diminishing-bias.


\begin{figure*}[tb]
{\small \hskip 2.5cm$\bs{\gamma=0}$\hskip 4cm$\bs{\gamma=2}$\hskip 4cm$\bs{\gamma=10}$}

~~~\includegraphics[width=6in]{graphs/sim_paths}
\caption{\label{simpaths} Regularization paths for simulation example.
Degrees of freedom $df^t$ are along the top. }
\vskip .3cm
{\small \hskip 2.5cm$\bs{\gamma=0}$\hskip 4cm$\bs{\gamma=2}$\hskip 4cm$\bs{\gamma=10}$}

~~~\includegraphics[width=6in]{graphs/sim_cv}
\caption{\label{simcv} 5-fold CV and AICc for a simulation example. 
Points-and-bars show mean OOS MSE $\pm 1$se.  }
\vspace{-.25cm}
\end{figure*}

\section{Hockey example}
\label{sec:nhl}


This section attempts to quantify the
performance of hockey players.  It extends  analysis in
\cite{gramacy_estimating_2013}.  The current version includes data about who
was on the ice for every goal in the National Hockey League (NHL) back to the
2002-2003 season, including playoffs.  The data
are in the {\tt gamlr} package; there are
69449 goals and 2439 players.


The logistic regression model of player contribution is, for goal $i$ in
season $s$ with away team $a$ and home team $h$,
\begin{align}\label{hockeymod}
\mr{logit}\left[\mr{p}(\text{home~team~scored~goal}~i)\right] \\= \alpha_0 +
\alpha_{sh} - \alpha_{sa} + \bm{u}_i'\bs{\phi} + \bm{x}_i'\bs{\beta}_0 +
\bm{x}_i'\bs{\beta}_s, \end{align}  Vector $\bm{u}_i$ holds indicators for
various special-teams scenarios (e.g., a home team power play), and
$\bs{\alpha}$ provides matchup/season specific intercepts. Vector $\bm{x}_i$
contains player effects: $x_{ij}=1$ if player $j$ was on the home team and on
ice for goal $i$, $x_{ij}=-1$ for away player $j$ on ice for goal $i$, and
$x_{ij}=0$ for everyone not on the ice.   Coefficient $\beta_{0j} +
\beta_{sj}$ is the season-$s$ effect of player $j$ on the log odds that, given
a goal has been scored, the goal was scored by their team.  These effects are
`partial' in that they control for who else was on the ice, special teams
scenarios, and team-season fixed effects -- a player's $\beta_{0j}$ or $\beta_{sj}$
only need be nonzero if that player effects play above or below the team
average for a given season.

We estimate gamma lasso paths of $\bs{\beta}$ for the model in
(\ref{hockeymod}) {\it with  $\bs{\alpha}$ and $\bs{\phi}$ left unpenalized}.
In contrast to the default for most analyses, our coefficient costs are {\it
not} scaled by covariate standard deviation.  Doing the usual standardization
would have favored players with little ice time.  The algorithm is run for
$\log_{10}\gamma = -5 \ldots 2$, plus the $\gamma=0$ lasso.\footnote{On this
data, $\gamma=\infty$ subset selection yields perfect separation and infinite
likelihood.}

Joint $[\gamma,\lambda]$ surfaces for AICc and BIC are in Figure \ref{nhlic}.
AICc favors denser models with low $\lambda$ but not-to-big $\gamma$,
while the BIC  prefers very sparse but relatively unbiased  models with large
$\lambda$ and small $\gamma$.  Both criteria are strongly adverse to any model
at $\gamma=100$, which is where timings explode in Figure
\ref{nhltime}.  Ten-fold CV results are shown in Figure
\ref{nhlcv} for $\gamma$ of 0, 1, and 10.  The OOS error minima are around the
same in each case -- average deviance slightly above 1.16 -- but errors
increase much faster away from optimality with larger $\gamma$.   We also see
that AICc selection is always between the CV.min and CV.1se selctions:  at
$\gamma=0$ AICc matches the CV.1se choice, while at $\gamma=10$ it has moved
right to the CV.1se  selection.  Our heuristic from Section \ref{sss}.2 might be
over-estimating $df$ for large-$\gamma$ models (especially under this very
collinear design), but one would also suspect that CV estimates of minimum
deviance are biased downward more dramatically for larger $\gamma$ than for
low-variance small-$\gamma$ estimators.



\begin{figure*}[tb]
\begin{center}\vskip -.5cm
~~~~~~~~\includegraphics[width=5in]{graphs/nhl_ic}
\caption{ \label{nhlic} Hockey example AICc and BIC surfaces, rising from white to black on log scale.
}
\end{center}
% \end{figure}
% \begin{figure}[htb]
\includegraphics[width=6.3in]{graphs/nhl_cv}
\caption{\label{nhlcv} Hockey example 10-fold CV: mean OOS deviance $\pm 1$se, with minimum-error and 1SE selection rules marked with black dotted lines, and solid orange line showing AICc selection. }
\end{figure*}


\begin{table*}[tb]\footnotesize\setstretch{1}
\makebox[\linewidth]{
\begin{tabular}{l|lcc|lcc|lcc|}
\multicolumn{1}{c}{} & \multicolumn{3}{l}{\normalsize \it ~~~lasso} & \multicolumn{3}{l}{\normalsize \it ~~~$\gamma=1$} &  \multicolumn{3}{l}{\normalsize \it ~~~$\gamma=10$}\\
%\multicolumn{1}{c}{} & \multicolumn{3}{r|}{} & \multicolumn{3}{r|}{} &  \multicolumn{3}{r|}{}\\
\multicolumn{1}{c}{}  &     &    PPM & PM &    &   PPM & PM        &   & PPM & PM \\
\cline{3-4}\cline{6-7}\cline{9-10}
1 & Ondrej Palat & 33.8 & 38 & Sidney Crosby & 29.2 & 52 & Sidney Crosby & 32.6 & 52 \\
2 & Sidney Crosby & 31.2 & 52 & Ondrej Palat & 29 & 38 & Jonathan Toews & 22.8 & 35 \\
3 & Henrik Lundqvist & 25.8 & 9 & Jonathan Toews & 21.4 & 35 & Joe Thornton & 22 & 34 \\
4 & Jonathan Toews & 24 & 35 & Joe Thornton & 21 & 34 & Anze Kopitar & 22 & 39 \\
5 & Andrei Markov & 23.1 & 34 & Andrei Markov & 20.9 & 34 & Andrei Markov & 20.7 & 34 \\
6 & Joe Thornton & 21.4 & 34 & Henrik Lundqvist & 19.8 & 9 & Alex Ovechkin & 18.1 & 16 \\
7 & Anze Kopitar & 20.6 & 39 & Anze Kopitar & 19.5 & 39 & Pavel Datsyuk & 16.6 & 13 \\
8 & Tyler Toffoli & 18.9 & 31 & Pavel Datsyuk & 16.1 & 13 & Ryan Getzlaf & 15.8 & 16 \\
9 & Pavel Datsyuk & 17.7 & 13 & Logan Couture & 15.9 & 29 & Henrik Sedin & 15.2 & 7 \\
10 & Ryan Nugent-hopkins & 17.4 & 18 & Alex Ovechkin & 15.8 & 16 & Marian Hossa & 14.9 & 21 \\
11 & Gabriel Landeskog & 16.6 & 36 & Marian Hossa & 14.4 & 21 & Alexander Semin & 14.7 & -1 \\
12 & Logan Couture & 16.5 & 29 & Alexander Semin & 14.2 & -1 & Jaromir Jagr & 14.5 & 28 \\
13 & Alex Ovechkin & 15.8 & 16 & Matt Moulson & 13.9 & 22 & Logan Couture & 14.2 & 29 \\
14 & Marian Hossa & 15.4 & 21 & Tyler Toffoli & 13.3 & 31 & Matt Moulson & 13.7 & 22 \\
15 & Alexander Semin & 14.8 & -1 & David Perron & 12.7 & 2 & Mikko Koivu & 13 & 12 \\
16 & Zach Parise & 14.7 & 21 & Mikko Koivu & 12.5 & 12 & Joe Pavelski & 12.6 & 33 \\
17 & Frans Nielsen & 13.5 & 8 & Frans Nielsen & 12.3 & 8 & Steven Stamkos & 12.6 & 24 \\
18 & Mikko Koivu & 13.4 & 12 & Ryan Getzlaf & 12.1 & 16 & Frans Nielsen & 12.5 & 8 \\
19 & Matt Moulson & 13.4 & 22 & Ryan Nugent-hopkins & 11.9 & 18 & Marian Gaborik & 12.3 & 29 \\
20 & David Perron & 13.1 & 2 & Jaromir Jagr & 11.8 & 28 & Zach Parise & 12.2 & 21 \\
\multicolumn{1}{c}{} & \multicolumn{3}{r|}{} & \multicolumn{3}{r|}{} &  \multicolumn{3}{r|}{}\\
 \multicolumn{1}{c}{} & \multicolumn{3}{r|}{\it 305 nonzero effects} & \multicolumn{3}{r|}{\it 204 nonzero effects} &  \multicolumn{3}{r|}{\it 64 nonzero effects}
\end{tabular}}
\caption{\label{nhleffects} Top 20 AICc selected player `partial plus-minus' (PPM) values for the 2013-2014 season, under $\gamma = 0,1,10$.  The number of nonzero player effects for each $\gamma$ are noted along the bottom.}
\end{table*}

The motivating application for this example was to devise a better versions of
hockey's `plus-minus' (PM) statistic: number of goals {\it for} minus {\it
against} each player's team while he is on the ice. To convert from player
effects $\beta_{0j} + \beta_{sj}$ to the scale of `plus/minus', we first state
that in absence of any other relevant information about each goal (not team,
home/away, etc), the probability a goal was scored by his team given player
$j$ is on ice becomes $p_j = e^{\beta_j}/(1+e^{\beta_j})$ and our `partial
plus/minus' (PPM) is \[ \mr{ppm}_j = N_j(p_j - (1-p_j)) = N_j(2p_j-1) \] where
$N_j$ is the  number of goals for which he was on-ice.  This measures 
quality and quantity of contribution, controlling for
confounding information, and lives on the same scale as PM.

Table \ref{nhleffects} contains the estimated PPM values for the 2013-2014
season under various $\gamma$ levels, using AICc selection.  We see that, even
if changing concavity ($\gamma$) has little effect on minimum CV errors,
larger $\gamma$  yield more sparse models and different conclusions about
player contribution. At the $\gamma=0$ lasso, there are 305 nonzero player
effects (individuals measurably different from their team's average ability)
and the list includes young players who have had very strong starts to their
careers.  For example, Ondrej Palat and Tyler Toffoli both played their first
full seasons in the NHL in 2013-2014.  As $\gamma$ increases to 1, these young
guys  drop in rank while more proven stars (e.g., Sidney Crosby and Jonathan
Toews) move up the list.  Finally, at $\gamma=10$ only big-name stars remain
amongst the 64 nonzero player effects.

\section{Discussion}
\label{discussion}


Concave penalized estimation in Big data, where exact solvers are too
computationally expensive, reduces largely to weighted-$L_1$ penalization.
This review has covered a number of topics that we think relevant for such
schemes.  Apart from the simulation study, we have not provided extensive
comparison of the many available weighting mechanisms. However, we feel that
path adaptation is an intuitively reasonable source of weights. In any case,
POSE has the advantage that using a regularization path to supply penalty
weights is computationally efficient. To scale for truly Big data, $L_1$
weights need be constructed at practically no cost on top of a standard lasso
run. Beyond {\tt gamlr} and marginal regression adaptive lasso, we have found
no other software for sparse diminishing bias estimation where this standard
is met.



\setstretch{1}
\bibliographystyle{chicago}
\bibliography{pose}

\end{document}
