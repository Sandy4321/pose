\documentclass[twoside]{article}
\usepackage{aistats2016}
%\usepackage[accepted]{aistats2016}

\usepackage{amssymb,amsmath,natbib,graphicx,amsthm,
  setspace,sectsty,anysize,times,dsfont,enumerate}

\usepackage[svgnames]{xcolor}

\usepackage{lscape,arydshln,relsize,rotating,multirow}
\usepackage{algorithm,algorithmic}


\newtheorem{prop}{\sc Proposition}[section]
\newtheorem{theorem}{\sc Theorem}[section]
\newtheorem{definition}{\sc Definition}[section]
\newtheorem{lemma}{\sc Lemma}[section]
\newtheorem{corollary}{\sc Corollary}[section]
\DeclareMathOperator*{\argmin}{argmin}

\marginsize{1.1in}{.9in}{.3in}{1.4in}

\newcommand{\nb}{\color{blue}}
\newcommand{\dbl}{\setstretch{1.5}}
\newcommand{\sgl}{\setstretch{1.2}}

\newcommand{\bs}[1]{\boldsymbol{#1}}
\newcommand{\mc}[1]{\mathcal{#1}}
\newcommand{\mr}[1]{\mathrm{#1}}
\newcommand{\bm}[1]{\mathbf{#1}}
\newcommand{\ds}[1]{\mathds{#1}}
\newcommand{\indep}{\perp\!\!\!\perp}


\begin{document}

\twocolumn[

\aistatstitle{One-step estimator paths for concave regularization}

\aistatsauthor{ Matt Taddy }

\aistatsaddress{The  University of Chicago Booth School of Business} ]

\begin{abstract}
The statistics literature of the past 15 years has established many favorable
 properties for sparse diminishing-bias regularization: techniques which can
 roughly be understood as providing estimation under penalty functions
 spanning the range of concavity between $L_0$ and $L_1$ norms.  However,
 lasso $L_1$-regularized estimation remains the standard tool for industrial
 `Big Data' applications because of its minimal computational cost and the
 presence of easy-to-apply rules for penalty selection.   In response, this
 article proposes a simple new algorithm framework that requires no more
 computation than a lasso path: the path of one-step estimators (POSE) does
 $L_1$ penalized regression estimation on a grid of decreasing penalties, but
 adapts coefficient-specific weights to decrease as a function of the
 coefficient estimated in the previous path step.  This provides sparse
 diminishing-bias regularization at no extra cost over the fastest lasso
 algorithms. Moreover, our `gamma lasso' implementation of POSE is accompanied
 by a reliable heuristic for the fit degrees of freedom, so that
 standard information criteria can be applied in penalty selection. The
 methods are illustrated in extensive simulations and in application of
 logistic regression to evaluating the performance of hockey players.
 \end{abstract}

\section{Introduction}
\label{intro}

For regression in high-dimensions, it is useful to regularize estimation
through a penalty on coefficient size.   $L_1$  regularization \citep[i.e.,
the lasso of][]{tibshirani_regression_1996} is especially popular, with costs
that are non-differentiable at their minima and can lead to  coefficient
solutions of exactly zero.  A related approach is concave penalized
regularization (e.g. SCAD from \citealt{fan_variable_2001} or MCP from
\citealt{zhang_nearly_2010}) with cost functions that are also spiked at zero
but flatten for large values (as opposed to the constant increase of an $L_1$
norm).  This yields sparse solutions where  large non-zero values are estimated
with little bias. 

The combination of  \textit{sparsity} and \textit{diminishing-bias} 
 is appealing in many settings, and a large literature on concave
penalized estimation has developed over the past 15 years.  For example, many
authors (e.g., from \citealt{fan_variable_2001} and
\citealt{fan_nonconcave_2004})  have contributed work on their \textit{oracle
properties}, a class of results showing conditions under which coefficient
estimates through concave penalization, or in related schemes, will be the
same as if you knew the sparse `truth' (either asymptotically or with high
probability).   From an information compression perspective,  the increased
sparsity encouraged by diminishing-bias penalties (since single large
coefficients are allowed to account for the signals of other correlated
covariates) leads to lower memory, storage, and communication requirements.
Such savings are very important in distributed  computing schemes
\citep[e.g.,][]{taddy_distributed_2013}.



Unfortunately,  exact solvers for concave penalized estimation  all require
significantly more compute time than a standard lasso.  In our experience,
this has precluded their use in settings -- e.g., in text or web-data analysis
-- where both $n$ (the number of observations) and $p$ (the number of
   covariates) are very large. As we review below,  recent literature
   recommends the use of approximate solvers. Certainly, this is necessary for
   data of the size we encounter in analysis of, say, internet commerce.
   These approximations  take the form of iteratively-weighted-$L_1$
   regularization, where the coefficient-specific weights are based upon
   estimates of the coefficients taken from previous iterations of the
   approximate solver.  This literature
\citep[e.g.,][]{zou_one-step_2008,fan_strong_2014} 
holds that even a single step of weighted-$L_1$ regularization is enough to
get solutions that are close to optimal, so long as the pre-estimates are
\textit{good enough} starting points. The crux of success with such one-step
estimation (OSE) is  finding starts that are, indeed, good enough.


This article provides a complete framework for sparse diminishing-bias
regularization that combines ideas from OSE with the concept of a
\textit{regularization path} -- a general technique, most famously associated
with the LARS algorithm \cite{efron_least_2004}, that estimates a sequence of
models under decreasing amounts of regularization.  So long as the model estimates do not change too much from one level of regularization to the next, such path algorithms can be very fast to run and are an efficient way to obtain a high-quality \textit{set} of models to choose amongst.  

A path of one-step estimators (POSE; detailed in Section \ref{pose})
algorithm does $L_1$ penalized regression estimation on a grid of decreasing
penalties, but adapts coefficient-specific weights to decrease as a function
of the coefficient estimated in the previous path step.  POSE takes advantage
of the natural match between path algorithms and one-step estimation: OSE
relies upon inputs being close to the optimal solution, which is precisely the
setting where path algorithms are most efficient.  This framework allows us to provide 
\begin{itemize}
\item a {\it path} of coefficient fits, each element of which corresponds to sparse diminishing-bias regularization estimation under a different level of penalization; where
\item obtaining the path of coefficient fits requires no more computation than  a state-of-the-art $L_1$ regularization pat algorithm; and
\item there are reliable closed-form rules for selection of the optimal penalty level along this path.
\end{itemize}
The last capability here is derived from a Bayesian interpretation for our \textit{gamma lasso} implementation of POSE (see Section \ref{sec:gamlr}) from which we are able to construct heuristic information criteria for penalty selection.
We view such penalty-selection tools as an essential ingredient for practical applicability in large-scale industrial machine learning where, e.g., cross-validation is not always viable or advisable.


The remainder of this paper is outlined as follows.  Section \ref{rp} is a
survey of sparse diminishing-bias regularization: we review the connection
between concave and weighted-$L_1$ penalties in
\ref{rp}.1, and present novel results on the distance between weighted-$L_1$
 and $L_0$ minimization in \ref{rp}.2. The POSE algorithm is introduced in
Section \ref{pose}, with our  gamma lasso specification in \ref{pose}.1 and its motivation from Bayesian foundations in \ref{pose}.2.  Section \ref{sss}  covers
implementation issues key to any path-based strategy:  speed and stability
in \ref{sss}.1, and model selection  in \ref{sss}.2.  Finally, Section
\ref{eg} presents two empirical studies:
a  simulation experiment in \ref{eg}.1, and in \ref{eg}.2 we investigate the data analysis question: given all goals in the past decade of NHL hockey, what can we say about individual player contributions? 

\section{Sparse regularization paths and diminishing bias}
\label{rp}

Denote $n$ response observations as $\bm{y} = [y_1,\ldots,y_n]'$ and the associated matrix of $p$ covariates as $\bm{X} =
[\bm{x}_1 \cdots \bm{x}_n]'$, with rows $\bm{x}_i = [x_{i1},\ldots,x_{ip}]'$ and columns $\bs{x}_j = [x_{1j},\ldots,x_{nj}]'$.\footnote{Since the size of penalized $\beta_j$ depends upon the units of $x_{ij}$,  it is common to scale
the coefficient by $\mr{sd}(\bs{x}_j)$, the standard deviation of the $j^{th}$ column
of $\bm{X}$; this is achieved if $x_{ij}$ is replaced by $x_{ij}/\mr{sd}(\bs{x}_j)$
throughout.} Write $\eta_{i} =
\alpha+\bm{x}_i'\bs{\beta}$ as the linear equation for observation $i$, and
denote with $l(\alpha, \bs{\beta}) = l(\bs{\eta})$  an unregularized
objective  proportional to the negative log likelihood.  For example, in Gaussian (linear)
regression, $l(\bs{\eta})$ is the sum-of-
squares $0.5\sum_i \left(y_i - \eta_i\right)^2$ and in binomial (logistic)
regression,  $l(\bs{\eta}) = -\sum_i \left[\eta_iy_i -
\log(1+e^{\eta_i})\right]$ for $y_i \in [0,1]$.  
A penalized estimator is then the solution to
\begin{equation} \label{pendev}
\argmin_{\alpha,\beta_j\in\ds{R}}~~\left\{~l(\alpha,{\bs{\beta}}) + n\lambda \sum_{j=1}^p c_j(|\beta_j|)~\right\},
\end{equation}
where $\lambda>0$ controls overall penalty magnitude and are $c_j()$ are coefficient cost functions.
 
\begin{figure*}[t]
\vspace{-.5cm}
\includegraphics[width=\textwidth]{graphs/penalties}
\vskip -.5cm
\caption{\label{costs} 
From left to right, 
$L_2$ costs \citep[ridge,][]{hoerl_ridge_1970}, $L_1$ \citep[lasso,][]{tibshirani_regression_1996}, the `elastic net' mixture of $L_1$ and $L_2$ \citep{zou_regularization_2005}, and the log penalty \citep{candes_enhancing_2008}.
}
\end{figure*}

A few common cost functions are shown in Figure \ref{costs}.  Those that have
a non-differentiable spike at zero (all but ridge) lead to sparse estimators,
with some coefficients set to exactly zero.   The curvature of the penalty
away from zero dictates then the weight of shrinkage imposed on the nonzero
coefficients:  $L_2$ costs increase with coefficient size,  lasso's $L_1$
penalty has zero curvature and imposes constant shrinkage, and as curvature
goes towards $-\infty$ one approaches the $L_0$ penalty of subset selection.
In this article we are primarily interested in {\it concave} cost functions,
like the log penalty, which span the range between $L_1$ and $L_0$ penalties.

The penalty size, $\lambda$, acts as a {\it squelch}: it suppresses noise to
focus on the true input signal. Large $\lambda$ lead to very simple 
model estimates, while as $\lambda \rightarrow 0$ we approach maximum
likelihood estimation (MLE). Since you don't know optimal $\lambda$,
practical application of penalized estimation requires a {\it regularization
path}: a $p \times T$ field of $\bs{\hat\beta}$ estimates obtained while
moving from high to low penalization along $\lambda^1 > \lambda^2 \ldots >
\lambda^T$ \citep[e.g., LARS in][is a well known example]{efron_least_2004}.
These paths begin at $\lambda^1$ set to infimum $\lambda$ such that
(\ref{pendev}) is minimized at $\bs{\hat\beta} = \bm{0}$ (see Appendix
\ref{models}), and proceed down to some pre-specified $\lambda^T$ (e.g., $\lambda^T=
0.01\lambda^1$).

\subsection{Concave penalization}


\begin{figure*}[t]
\vskip -.25cm
\includegraphics[width=\textwidth]{graphs/solution}
\caption{\label{solution} Log penalties $c(\beta) = s\log(1 + \gamma|\beta|)$ 
and penalized objectives $(\beta-B)^2 + c(\beta)$.}
\end{figure*}


Concave penalties such as the log penalty, which have a gradient that is
decreasing with absolute coefficient size,  yield the `diminishing-bias'
property discussed in the introduction. It is {\it the} reason why one would
use concave penalization instead of $L_1$ or convex alternatives.
Unfortunately, such penalties can overwhelm the convex likelihood and produce a 
concave minimization objective; see Figure \ref{solution} for illustration.


This concavity makes computation difficult.  For example,  one run of SCAD via
the \texttt{ncvreg} R package \citep{breheny_coordinate_2011} for the
simulation in Section \ref{eg}.1 requires around 10 minutes, compared to 1-2
seconds for the lasso.  The slowness is due to the difficulty of concave
minimization: finding global, or even good local, minima is an extremely tough
task.  The most efficient exact solver that we've found is the
\texttt{sparsenet} of \citet{mazumder_sparsenet_2011}, also implemented in R,
which first fits a lasso path under decreasing $L_1$ penalty magnitude and,
for each segment on this path, adapts coefficient estimates along a second
path of increasing penalty concavity. While far more efficient than previous
alternatives, \texttt{sparsenet}  relies upon solution over a large set of
penalty specifications\footnote{POSE shares with
\texttt{sparsenet} the idea of moving along a path of closely related
specifications, but does not require a grid in both  cost size and concavity.
Intuitively,  POSE runs  a path diagonally through this grid.}  
 and its compute cost remains much higher than for lasso
(e.g., 15-20 seconds in our simulation).

A relatively fast class of solvers for concave penalized estimators uses
local linear approximation \cite[LLA; e.g.,][]{candes_enhancing_2008}. LLA
replaces the concave cost function $c_j$  with its tangent at the current
estimate, $c_j'(\hat\beta_j)\beta_j$.  The objective is then just a weighted
$L_1$ penalized loss (solvable, say, as in Appendix \ref{implement}), and one
iterates between updating $c'(\hat\beta)$ and solving the implied $L_1$
penalized minimization problem.  \citet{zou_one-step_2008} present numerical
and theoretical evidence that  LLA does well in practice. Importantly, they
show that LLA does well even (or especially) if you {\it stop it after one
iteration}. This is an example of one-step estimation (OSE), a 
technique inspired by \cite{bickel_one-step_1975} that amounts to taking as
your estimator the first step of an iterative approximation to some objective.
Such early-stopping can be as good  as the full-step solution
{\it if} the initial estimates are good enough. 

OSE and similar ideas have had a resurgence in the concave penalization
literature recently, motivated by the need for faster estimation algorithms.
\cite{fan_strong_2014} consider early-stopping of LLA for folded concave
penalization  and show that, under strong sparsity assumptions about true
$\bs{\beta}$ and given appropriate initial values, OSE LLA is with high
probability an oracle estimator.   Zhang (2010,2013)
\nocite{zhang_analysis_2010,zhang_multi-stage_2013} investigates  
`convex relaxation' iterations, where estimates under convex regularization
 are the basis for weights in a subsequent penalized objective.  He shows that
 just one or two steps here is sufficient for obtaining oracle support
 recovery properties under much weaker conditions than required by a standard
 lasso.  \cite{wang_calibrating_2013} propose a two step algorithm that feeds
 lasso coefficients into a linear approximation to folded concave
 penalization.  These OSE methods are all closely related to  the adaptive lasso
\citep[AL;][]{zou_adaptive_2006}, which does weighted-$L_1$ minimization under
weights $\omega_j = 1/|\hat\beta^0_j|$, where $\hat\beta^0_j$ is an initial
guess at the coefficient value.  The original AL paper advocates using MLE
estimates for initial values, while
\cite{huang_adaptive_2008} suggest using marginal regression coefficients
$\hat\beta^0_j = \mr{cor}(\bm{x}_j,\bm{y})$.\footnote{We include marginal AL in our study
of Section \ref{eg}.1.}

OSE LLA, or a two-step estimator starting from
$\bs{\hat\beta}=0$ as suggested in \cite{fan_strong_2014} and
\cite{wang_calibrating_2013}, or any version of the adaptive lasso, are {\it all just weighted-$L_1$
minimization}. These methods differ only in how the
weights are constructed.  Regardless of which you prefer, weighted-$L_1$
penalties are likely to play a central role in diminishing-bias penalization
whenever the dataset is too large for use of exact concave penalty solvers.



\section{Path of one-step estimators}
\label{pose}

Our path of one-step estimators (POSE), in Algorithm \ref{posealgo}, uses solutions along the sequence of decreasing penalty sizes, $\lambda^t$, as the basis for LLA weights at the next path step.  In this, we are assuming a penalty specification such that $\lim_{b\to 0} c'(|b|) = 1$ and that the cost function is differentiable for $b\neq 0$.  This yields a path of one-step LLA
penalized coefficient estimates. 

\vskip .2cm
{
\begin{algorithm}[ht]
\caption{\label{posealgo} POSE }
\vskip .2cm
Initialize $\bs{\hat\beta}^0 = \bf{0}$, so that $\hat S_0 = \varnothing$.

\vskip .2cm
Set $\lambda^1 >
0$ with step size
$0 < \delta < 1$.

\vspace{-.75cm}
\begin{align}
\text{for}~t=1\ldots T :&\notag \\
\omega^{t}_j  &=  
\left\{ 
  \begin{array}{r}
    c'(|\hat\beta^{t-1}_j|) ~\text{for}~j \in \hat S_t \\
    1  ~\text{for}~j \in \hat S_t^c  
  \end{array} 
  \right. 
  \label{wset}\\
\left[\hat\alpha,\bs{\hat\beta}\right]^t &= \argmin_{\alpha,\beta_j\in\ds{R}}~~
l(\alpha,\bs{\beta}) + n\sum_j \lambda^t\omega^t_j|\beta_j| \label{l1pen}\\
\lambda^{t+1} &= \delta \lambda^t\notag
\end{align}
\vskip -.3cm
\end{algorithm}}

 From an engineering standpoint, POSE has the same appeal as any successful path algorithm: if the estimates change little from iteration $t$ to $t+1$, then you will be able to quickly solve for a large set of candidate specifications.  Following the discussion of Section \ref{rp}.1, such algorithms are a natural match with one-step estimation: OSE relies upon inputs being close to the optimal solution, which is precisely the setting where path algorithms are most efficient.  More rigorously, Theorem \ref{sparseapprox} applied to POSE yields  $\hat S_{t-1} \cap S^c = \varnothing \Rightarrow \omega_{S^c}^{t~\mr{min}} = 1$.  Thus so long as $\lambda$ is large enough,  Section \ref{rp}.2 demonstrates that  fast diminishing $\omega_j$ will help control false discovery and improve prediction.  Of course, the moment $\hat S_t \cap S^c \neq \varnothing$,  diminishing-bias allows spurious covariates to enter with  little shrinkage and can move the fit arbitrarily far away from $L_0$-optimality -- that is, with $\lambda$ too small the diminishing bias hurts your ability to estimate and predict.  This is why it is essential to have a path of candidate $\lambda^t$ to choose amongst.


\section{Sparse regularization paths and diminishing bias}
\label{rp}




\section{The gamma lasso}
\label{sec:gamlr}

The gamma lasso (GL) specification for POSE is based upon the log penalty,
\begin{equation}\label{logpen}
c(\beta_j) =  \log(1+\gamma|\beta_j|),
\end{equation} where $\gamma > 0$.  This
penalty is concave with curvature $-1/(\gamma^{-1}+|\beta_j|)^2$ and it spans the
range from $L_0$ ($\gamma\rightarrow \infty$) to $L_1$ ($\gamma\rightarrow 0$)
costs (see Figure \ref{solution}).  It appears under a variety of parameterizations and names in the
literature; see
\citet{mazumder_sparsenet_2011} and applications in
\citet{friedman_fast_2008}, \citet{candes_enhancing_2008},
\citet{cevher_learning_2009}, \citet{taddy_multinomial_2013} and \citet{armagan_generalized_2013}. 

GL -- POSE under the log penalty  -- leads to line (\ref{wset}) being replaced by
\begin{equation}\label{gammalasso}
\omega^{t}_j  = \left(1 + \gamma |\hat\beta^{t-1}_j|\right)^{-1} ~~j=1\ldots p 
\end{equation}
Behavior of the resulting paths  is governed by $\gamma$, which we refer to as the
penalty {\it scale}.  Under $\gamma=0$, GL is just the usual lasso.
 Bias diminishes faster for larger $\gamma$ and, at the  extreme,
$\gamma=\infty$ yields a subset selection routine where a coefficient is
unpenalized in all segments after it first becomes nonzero. Figure
\ref{gamlr_eg} shows solutions in a simple problem.


\begin{figure*}[t]
\hskip -.2cm\includegraphics[width=6.5in]{graphs/gamlr_eg}
\vskip -.25cm
\caption{\label{gamlr_eg} Gamma lasso estimation on $n=10^3$ 
 observations of $y_i = 4 + 3x_{1i} - x_{2i} + \varepsilon_i$, where
$\varepsilon_i \stackrel{ind}{\sim} \mr{N}(0,1)$ and
$\{x_{1i},x_{2i},x_{3i}\}$ are marginally standard normal with correlation of
0.9 between covariates ($x_{3i}$ is spurious). The penalty path has $T=100$
segments, $\lambda^1 = n^{-1}\left| \sum_i x_{1i}y_i\right|$, and
$\lambda^{100} = 0.01\lambda^1$. Degrees of freedom are on top and vertical
lines mark AICc and BIC selected models (see Section
\ref{sss}).}
\vskip -.25cm
\end{figure*}

Each  gamma lasso path segment is solved through coordinate descent,
as detailed in Appendix \ref{implement}.  The algorithm is implemented in {\tt
c} as part of the {\tt gamlr} package for {\sf R}. The software  has detailed documentation and versioned source code is at
{\tt github.com/mataddy/gamlr}.  Usage of {\tt gamlr} mirrors that of its
convex penalty analogue {\tt glmnet} \citep{friedman_regularization_2010}, the
fantastic and widely used package for costs between $L_1$ and $L_2$ norms. In
the lasso case ($\gamma=0$), the two algorithms are essentially equivalent.


\subsection{Bayesian motivation}

Consider a model where each $\beta_j$ is
assigned a Laplace distribution prior with scale $\tau_j>0$,
\begin{equation}\label{blasso}
\beta_j \sim \mr{La}\left(\tau_j\right) =
\frac{\tau_j}{2}\exp\left[ -\tau_j|\beta_j| ~\right].
\end{equation}
Typically, scale parameters $\tau_1 =
\ldots = \tau_p$ are set as a single shared value, say $n\lambda/\phi$ where
 $\phi$ is the exponential family dispersion (e.g. Gaussian variance
$\sigma^2$ or 1 for the binomial).   Posterior
maximization under the prior in (\ref{blasso}) is then lasso estimation \citep[e.g.,][]{park_bayesian_2008}.

Instead of working from shared scale, assume an independent gamma
$\mr{Ga}(s,1/\gamma)$ hyperprior with `shape' $s$ and `scale' $\gamma$ for
each $\tau_j$, such that $\ds{E}[\tau_j] = s\gamma$ and $\mr{var}(\tau_j) =
s\gamma^2$.  Then the {\it joint} prior for both coefficient and scale is
\begin{equation}\label{glprior}
\pi(\beta_j,\tau_j) = \mr{La}\left(\beta_j ;~ \tau_j\right)
\mr{Ga}\left(\tau_j;~ s,\gamma^{-1}\right) = \frac{ 1}{2\Gamma({s})} 
\left(\frac{\tau_j}{\gamma}\right)^{s}
               \exp\left[-\tau_j(\gamma^{-1}+|\beta_j|)\right].
\end{equation}
The gamma hyperprior is conjugate here, implying a $\mr{Ga}\left(s+1, ~1/\gamma +
|\beta_j|\right)$ posterior for $\tau_j \mid \beta_j$ with conditional
posterior mode (MAP) at $\hat\tau_j = \gamma s/(1 + \gamma |\beta_j|)$.

Consider joint MAP estimation of $[\bs{\tau},\bs{\beta}]$ under the prior in
   (\ref{glprior}), where we've suppressed $\alpha$ for simplicity. By taking
   negative logs and removing constants, this is equivalent to solving
\begin{equation}\label{gljoint}
\min_{\beta_j\in\ds{R},~\tau_j \in \ds{R}^{+}}~~
\phi^{-1}l(\bs{\beta}) + \sum_j \left[\tau_j(\gamma^{-1}+|\beta_j|) - s\log(\tau_j)\right].
\end{equation}
It is straightforward to show that (\ref{gljoint}) is equivalent 
to the log-penalized objective 
\begin{equation}\label{logobj}
\min_{\beta_j\in\ds{R}}~~
\phi^{-1}l(\bs{\beta}) + \sum_j  s\log(1+\gamma|\beta_j|)
\end{equation}

\begin{prop}\label{penprop}
  $\bs{\hat\beta}$ solves (\ref{logobj}) if and only if it is also in the
  solution to (\ref{gljoint}).
\end{prop}
\begin{proof}
  The conditional posterior mode for each $\tau_j$ given $\beta_j$
  is $\tau(\beta_j) = \gamma s/(1 + \gamma|\beta_j|)$.  Any joint solution
  $[\bs{\hat\beta},\bs{\hat\tau}]$ for (\ref{gljoint}) thus
  consists of $\hat{\tau}_{j} = \tau(\hat\beta_{j})$;
  otherwise, it is always possible to decrease the objective by
  replacing $\hat\tau_{j}$. Setting each $\tau_j = 
  \tau(\beta_j)$ in (\ref{gljoint}) and removing constant terms yields
  (\ref{logobj}).  Moreover, the solution to (\ref{gljoint}) solves
  (\ref{logobj}): otherwise, there would need to be a point on the profile
  slice of (\ref{gljoint}) defined by $\tau_{j} =
  \tau(\hat\beta_{j})$ that is lower than its minimum.
\end{proof}

For a Bayesian it is odd to be solving for $\bs{\tau}$ rather than
marginalizing over its uncertainty.  However, recognizing the form of a gamma
density  in (\ref{glprior}), $\pi(\beta_j,\tau_j)$ integrates over $\tau_j$ to
yield the marginal prior $ \pi(\beta_j) = 0.5s\left( 1+
\gamma|\beta_j|\right)^{-(s+1)}$. This is the generalized double Pareto
density, as in  \citet{armagan_generalized_2013}. Since $-\log \pi(\beta_j)
\propto (s+1)\log(1 + \gamma|\beta_j|)$, the {\it profile} MAP solution to
(\ref{gljoint}) is also the {\it marginal} MAP for
$\bs{\beta}$ under $\mr{Ga}(s-1,1/\gamma)$ priors on each $\tau_j$.


\setstretch{1}
\bibliographystyle{chicago}
\bibliography{pose}

\end{document}
